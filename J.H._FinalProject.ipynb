{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMu2HQ/rVfV4YtQzvvJjvTP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiachoi-ds/Reinforcement-Learning/blob/Ji-Hyeon-Yoo/J.H._FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K7c2hdPlmgah"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "\n",
        "from gym import spaces\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyEnv(gym.Env):\n",
        "\n",
        "    metadata = {\"render.modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self, K_min=1000.0, K_max=20000.0, max_steps=200):\n",
        "        super().__init__()\n",
        "\n",
        "        # state(obs): q, L, C, T, X, xI_cur, xH_cur\n",
        "        # there are no done\n",
        "        obs_low = np.array([0.0,    0.0,   0.0,   0.0,   0.0, 0.0, 0.0], dtype=np.float32)\n",
        "        obs_high = np.array([1.0, 50000., 50000., 20000., 50.0, 1.0, 1.0], dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(obs_low, obs_high, dtype=np.float32) # define for obs space, and for using PPO (caution: string match)\n",
        "\n",
        "        # Government Cap\n",
        "        self.K_min = K_min\n",
        "        self.K_max = K_max\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([self.K_min], dtype=np.float32), high=np.array([self.K_max], dtype=np.float32), dtype=np.float32) # define for action space, and using PPO (caution: string match)\n",
        "\n",
        "        # weight parameter alpha for compute UG\n",
        "        self.alpha_1 = 1.0 #for insurance company\n",
        "        self.alpha_2 = 1.0 #for homeowner\n",
        "\n",
        "        self.m = 500.0 # risk premium for Insurance Company\n",
        "\n",
        "        # state transition hyper parameters\n",
        "        self.q_bar = 0.04 #default(usual expect) probability for fire probability\n",
        "        self.L_bar = 10000.0 #default(usual expect) loss when fire outbreak\n",
        "        self.C_bar = 5000.0 #\n",
        "\n",
        "        # momentum(mean-reversion) coefficient\n",
        "        self.phi_q = 0.9\n",
        "        self.phi_L = 0.9\n",
        "        self.phi_C = 0.9\n",
        "\n",
        "        # noise scaling parameter(effect on momentum)\n",
        "        self.sigma_q = 0 #0.002\n",
        "        self.sigma_L = 0 #500.0\n",
        "        self.sigma_C = 0 #100.0\n",
        "\n",
        "        ## noise from gaussian distribution\n",
        "        # self.noise_q = random.randomm()\n",
        "        # self.noise_L = random.randomm()\n",
        "        # self.noise_C = random.randomm()\n",
        "        ## >> redundant while using step()!\n",
        "\n",
        "        ## dynamic parameter of market status. Especcially for 'when a insurance company puts a product on the market'\n",
        "        # # it'll change while step() goes on, just initial settings as float.\n",
        "        # self.delta_C_pos = 150.0\n",
        "        # self.delta_C_neg = 80.0\n",
        "        ## >> no use while using step()\n",
        "\n",
        "        # extra management cost increase ratio for Insurance company.\n",
        "        self.gamma_T = 50.0\n",
        "\n",
        "        # satisfying differential ratio parameter for home owner\n",
        "        self.gamma_X = 0.2\n",
        "        self.rho_bar = 0.8 # goal coverage for home owner, never set as 1\n",
        "\n",
        "        self.max_steps = max_steps\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.state = None # set whatever you want as initial. I'll just leave it to reset()\n",
        "        self.last_info = {}\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def reset(self):\n",
        "        q_0 = np.clip(np.random.normal(self.q_bar, 0.005), 0.001, 0.2)\n",
        "        L_0 = np.random.normal(self.L_bar, 1000.0)\n",
        "        C_0 = np.random.normal(self.C_bar, 300.0)\n",
        "        T_0 = 1000.0\n",
        "        X_0 = 5.0\n",
        "        xI_cur = 0.0 # 0.0 or 1.0\n",
        "        xH_cur = 0.0 # 0.0 or 1.0\n",
        "\n",
        "        self.state = np.array( [q_0, L_0, C_0, T_0, X_0, xI_cur, xH_cur], dtype=np.float32)\n",
        "        self.step_count = 0\n",
        "        self.last_info = {}\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        # _cur : current state indexing label\n",
        "        # _next: next state indexing label\n",
        "\n",
        "        # action: government cap\n",
        "        K_cur = float(np.clip(action[0], self.K_min, self.K_max))\n",
        "\n",
        "        # unpack the state\n",
        "        q_cur, L_cur, C_cur, T_cur, X_cur, xI_cur, xH_cur = self.state #set xI and xH as previousone for easy-comprehension\n",
        "\n",
        "        # insurer premium decision (capped)\n",
        "        P_cur = min(K_cur, q_cur * C_cur + self.m)\n",
        "\n",
        "        # homeowner decision making (buy or not)\n",
        "        desire_buy = 1.0 if P_cur <= (q_cur * C_cur + X_cur) else 0.0\n",
        "\n",
        "        # expected profit for insurance company when homeowner buys\n",
        "        exp_profit_if_buy = desire_buy * (P_cur - q_cur * C_cur) - T_cur\n",
        "\n",
        "        if exp_profit_if_buy >= 0:\n",
        "            xI_next = 1.0\n",
        "            xH_next = desire_buy  # homeowner actually buys if it wanted to\n",
        "        else:\n",
        "            xI_next = 0.0\n",
        "            xH_next = 0.0  # no insurer -> no purchase\n",
        "\n",
        "        # compute utilities\n",
        "        U_I = xI_next * (xH_next * (P_cur - q_cur * C_cur) - T_cur)\n",
        "        U_H = xI_next * xH_next * (q_cur * C_cur - P_cur + X_cur) - q_cur * L_cur\n",
        "        reward = self.alpha_1 * U_I + self.alpha_2 * U_H\n",
        "\n",
        "\n",
        "        # state transition\n",
        "        # fire probability (mean reversion + noise)\n",
        "        q_next = self.phi_q * q_cur + (1 - self.phi_q) * self.q_bar + self.sigma_q * np.random.randn()\n",
        "        q_next = float(np.clip(q_next, 0.001, 0.5))\n",
        "\n",
        "        # loss size\n",
        "        L_next = self.phi_L * L_cur + (1 - self.phi_L) * self.L_bar + self.sigma_L * np.random.randn()\n",
        "        L_next = float(max(100.0, L_next))\n",
        "\n",
        "        # cost of homeowner\n",
        "        C_base = self.phi_C * C_cur + (1 - self.phi_C) * self.C_bar\n",
        "\n",
        "        # market-response scale parameter\n",
        "\n",
        "        up_ratio = 0.05 # no entry -> cost up by 5%\n",
        "        down_ratio = 0.03 # entry -> cost down by 3%\n",
        "\n",
        "        if xI_next == 0.0:\n",
        "            C_next = C_base + up_ratio * C_cur\n",
        "        else:\n",
        "            C_next = C_base - down_ratio * C_cur\n",
        "\n",
        "        # optional stochastic noise on C\n",
        "        if self.sigma_C > 0.0:\n",
        "            C_next += self.sigma_C * np.random.randn()\n",
        "\n",
        "        C_next = float(max(100.0, C_next))\n",
        "\n",
        "        # 7-4. operating cost\n",
        "        T_next = T_cur + self.gamma_T * (1.0 - xI_next)\n",
        "\n",
        "        # 7-5. psychological utility\n",
        "        X_next = X_cur + self.gamma_X * (xH_next - self.rho_bar)\n",
        "        X_next = float(np.clip(X_next, 0.0, 50.0))\n",
        "\n",
        "        # 7-6. pack next state\n",
        "        self.state = np.array(\n",
        "            [q_next, L_next, C_next, T_next, X_next, xI_next, xH_next],\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # truncation case\n",
        "        done = self.step_count >= self.max_steps\n",
        "\n",
        "        return self.state, float(reward), done\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "      print(\"need to be implemented\")\n",
        "      return\n"
      ],
      "metadata": {
        "id": "-bnrNVPlmjbf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = MyEnv()\n",
        "obs = env.reset()\n",
        "\n",
        "num_steps = 10\n",
        "for t in range(num_steps):\n",
        "    action = env.action_space.sample()\n",
        "    next_obs, reward, done = env.step(action)\n",
        "\n",
        "    print(f\"\\n[STEP {t+1}]\")\n",
        "    print(f\"Action (K): {action}\")\n",
        "    print(f\"Reward: {reward:.3f}\")\n",
        "    print(f\"Next state: {next_obs}\")\n",
        "\n",
        "    if done:\n",
        "        print(\"\\nEpisode finished early (max_steps reached).\")\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rNs7wHi6F06",
        "outputId": "8bc10b8c-681b-4ef1-a0da-45bc25d03ecd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[STEP 1]\n",
            "Action (K): [6792.655]\n",
            "Reward: -413.257\n",
            "Next state: [3.9705385e-02 1.0375014e+04 5.6196143e+03 1.0500000e+03 4.8400002e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 2]\n",
            "Action (K): [13398.543]\n",
            "Reward: -411.944\n",
            "Next state: [3.9734844e-02 1.0337512e+04 5.8386338e+03 1.1000000e+03 4.6800003e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 3]\n",
            "Action (K): [8122.4663]\n",
            "Reward: -410.759\n",
            "Next state: [3.9761361e-02 1.0303761e+04 6.0467021e+03 1.1500000e+03 4.5200005e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 4]\n",
            "Action (K): [5348.85]\n",
            "Reward: -409.692\n",
            "Next state: [3.9785225e-02 1.0273385e+04 6.2443667e+03 1.2000000e+03 4.3600006e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 5]\n",
            "Action (K): [14809.225]\n",
            "Reward: -408.729\n",
            "Next state: [3.9806701e-02 1.0246046e+04 6.4321479e+03 1.2500000e+03 4.2000008e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 6]\n",
            "Action (K): [8515.467]\n",
            "Reward: -407.861\n",
            "Next state: [3.9826032e-02 1.0221441e+04 6.6105405e+03 1.3000000e+03 4.0400009e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 7]\n",
            "Action (K): [17279.154]\n",
            "Reward: -407.079\n",
            "Next state: [3.9843429e-02 1.0199297e+04 6.7800132e+03 1.3500000e+03 3.8800008e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 8]\n",
            "Action (K): [12313.999]\n",
            "Reward: -406.375\n",
            "Next state: [3.9859086e-02 1.0179367e+04 6.9410122e+03 1.4000000e+03 3.7200007e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 9]\n",
            "Action (K): [18459.271]\n",
            "Reward: -405.740\n",
            "Next state: [3.9873179e-02 1.0161431e+04 7.0939614e+03 1.4500000e+03 3.5600007e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n",
            "\n",
            "[STEP 10]\n",
            "Action (K): [9154.729]\n",
            "Reward: -405.169\n",
            "Next state: [3.9885860e-02 1.0145287e+04 7.2392632e+03 1.5000000e+03 3.4000006e+00\n",
            " 0.0000000e+00 0.0000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaJ27R5L6KWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}